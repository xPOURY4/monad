#include <monad/mpt/trie.hpp>

#include <monad/async/concepts.hpp>
#include <monad/async/config.hpp>
#include <monad/async/erased_connected_operation.hpp>
#include <monad/async/io_senders.hpp>
#include <monad/core/assert.h>
#include <monad/core/byte_string.hpp>
#include <monad/core/nibble.h>
#include <monad/mem/allocators.hpp>
#include <monad/mpt/config.hpp>
#include <monad/mpt/nibbles_view.hpp>
#include <monad/mpt/node.hpp>
#include <monad/mpt/request.hpp>
#include <monad/mpt/state_machine.hpp>
#include <monad/mpt/update.hpp>
#include <monad/mpt/upward_tnode.hpp>
#include <monad/mpt/util.hpp>

#include <algorithm>
#include <bit>
#include <cassert>
#include <cstddef>
#include <cstdint>
#include <cstring>
#include <limits>
#include <memory>
#include <optional>
#include <span>
#include <tuple>
#include <utility>

MONAD_MPT_NAMESPACE_BEGIN

using namespace MONAD_ASYNC_NAMESPACE;

/* Names: `prefix_index` is nibble index in prefix of an update,
 `old_prefix_index` is nibble index of path in previous node - old.
 `*_prefix_index_start` is the starting nibble index in current function frame
*/
void dispatch_updates_flat_list_(
    UpdateAux &, StateMachine &, UpwardTreeNode &parent, ChildData &,
    Node::UniquePtr old, Requests &, NibblesView path, unsigned prefix_index);

void dispatch_updates_impl_(
    UpdateAux &, StateMachine &, UpwardTreeNode &parent, ChildData &,
    Node::UniquePtr old, Requests &, unsigned prefix_index, NibblesView path,
    std::optional<byte_string_view> opt_leaf_data);

void mismatch_handler_(
    UpdateAux &, StateMachine &, UpwardTreeNode &parent, ChildData &,
    Node::UniquePtr old, Requests &, NibblesView path,
    unsigned old_prefix_index, unsigned prefix_index);

void create_new_trie_(
    UpdateAux &aux, StateMachine &sm, ChildData &entry, UpdateList &&updates,
    unsigned prefix_index = 0);

void create_new_trie_from_requests_(
    UpdateAux &, StateMachine &, ChildData &, Requests &, NibblesView path,
    unsigned prefix_index, std::optional<byte_string_view> opt_leaf_data);

void upsert_(
    UpdateAux &, StateMachine &, UpwardTreeNode &parent, ChildData &,
    Node::UniquePtr old, chunk_offset_t offset, UpdateList &&,
    unsigned prefix_index = 0, unsigned old_prefix_index = 0);

void create_node_compute_data_possibly_async(
    UpdateAux &, StateMachine &, UpwardTreeNode &parent, ChildData &,
    tnode_unique_ptr, bool might_on_disk = true);

void compact_(
    UpdateAux &, StateMachine &, CompactTNode *parent, unsigned index, Node *,
    bool cached, chunk_offset_t node_offset = INVALID_OFFSET);

void try_fillin_parent_with_rewritten_node(
    UpdateAux &, CompactTNode::unique_ptr_type);

struct async_write_node_result
{
    chunk_offset_t offset_written_to;
    unsigned bytes_appended;
    erased_connected_operation *io_state;
};

// invoke at the end of each block upsert
chunk_offset_t write_new_root_node(UpdateAux &, Node &);

Node::UniquePtr upsert(
    UpdateAux &aux, StateMachine &sm, Node::UniquePtr old, UpdateList &&updates)
{

    aux.reset_stats();
    auto sentinel = make_tnode(1 /*mask*/, 0 /*prefix_index*/);
    ChildData &entry = sentinel->children[0];
    sentinel->children[0] = ChildData{.branch = 0};
    if (old) {
        upsert_(
            aux,
            sm,
            *sentinel,
            entry,
            std::move(old),
            INVALID_OFFSET,
            std::move(updates));
        if (sentinel->npending) {
            aux.io->flush();
            MONAD_ASSERT(sentinel->npending == 0);
        }
    }
    else {
        create_new_trie_(aux, sm, entry, std::move(updates));
    }
    auto *const root = entry.ptr;
    if (aux.is_on_disk()) {
        if (root) {
            write_new_root_node(aux, *root);
        }
        aux.print_update_stats();
    }
    return Node::UniquePtr{root};
}

/////////////////////////////////////////////////////
// Async read and update
/////////////////////////////////////////////////////

// Upward update until a unfinished parent node. For each tnode, create the
// trie Node when all its children are created
void upward_update(UpdateAux &aux, StateMachine &sm, UpwardTreeNode *tnode)
{
    while (!tnode->npending && tnode->parent) {
        MONAD_ASSERT(tnode);
        MONAD_DEBUG_ASSERT(tnode->children.size()); // not a leaf
        auto *parent = tnode->parent;
        auto &entry = parent->children[tnode->child_index()];
        // put created node and compute to entry in parent
        size_t const level_up =
            tnode->path.nibble_size() + !parent->is_sentinel();
        create_node_compute_data_possibly_async(
            aux, sm, *parent, entry, tnode_unique_ptr{tnode});
        sm.up(level_up);
        tnode = parent;
    }
}

struct update_receiver
{
    UpdateAux *aux;
    std::unique_ptr<StateMachine> sm;
    UpdateList updates;
    UpwardTreeNode *parent;
    ChildData &entry;
    chunk_offset_t rd_offset;
    unsigned bytes_to_read;
    uint16_t buffer_off;
    uint8_t prefix_index;

    update_receiver(
        UpdateAux *aux, std::unique_ptr<StateMachine> sm, ChildData &entry,
        chunk_offset_t offset, UpdateList &&updates, UpwardTreeNode *parent,
        unsigned const prefix_index)
        : aux(aux)
        , sm(std::move(sm))
        , updates(std::move(updates))
        , parent(parent)
        , entry(entry)
        , rd_offset(round_down_align<DISK_PAGE_BITS>(
              aux->virtual_to_physical(offset)))
        , prefix_index(static_cast<uint8_t>(prefix_index))
    {
        // prep uring data
        buffer_off = uint16_t(offset.offset - rd_offset.offset);
        // spare bits are number of pages needed to load node
        auto const num_pages_to_load_node = rd_offset.spare;
        MONAD_DEBUG_ASSERT(
            num_pages_to_load_node <=
            round_up_align<DISK_PAGE_BITS>(Node::max_disk_size));
        bytes_to_read =
            static_cast<unsigned>(num_pages_to_load_node << DISK_PAGE_BITS);
        MONAD_ASSERT(bytes_to_read <= AsyncIO::READ_BUFFER_SIZE);
        rd_offset.spare = 0;
    }

    void set_value(
        erased_connected_operation *,
        read_single_buffer_sender::result_type buffer_)
    {
        MONAD_ASSERT(buffer_);
        auto &buffer = buffer_.assume_value().get();
        auto old = deserialize_node_from_buffer(
            (unsigned char *)buffer.data() + buffer_off,
            buffer.size() - buffer_off);
        buffer.reset();
        // continue recurse down the trie starting from `old`
        unsigned const old_prefix_index = old->path_start_nibble();
        upsert_(
            *aux,
            *sm,
            *parent,
            entry,
            std::move(old),
            INVALID_OFFSET,
            std::move(updates),
            prefix_index,
            old_prefix_index);
        sm->up(1);
        upward_update(*aux, *sm, parent);
    }
};

static_assert(sizeof(update_receiver) == 64);
static_assert(alignof(update_receiver) == 8);

struct read_single_child_receiver
{
    UpdateAux *aux;
    chunk_offset_t rd_offset;
    UpwardTreeNode *tnode; // single child tnode
    ChildData &child;
    unsigned bytes_to_read;
    uint16_t buffer_off;
    std::unique_ptr<StateMachine> sm;

    read_single_child_receiver(
        UpdateAux *const aux, std::unique_ptr<StateMachine> sm,
        UpwardTreeNode *const tnode, ChildData &child)
        : aux(aux)
        , rd_offset(0, 0)
        , tnode(tnode)
        , child(child)
        , sm(std::move(sm))
    {
        // prep uring data
        // translate virtual offset to physical offset on disk for read
        auto const offset = aux->virtual_to_physical(child.offset);
        rd_offset = round_down_align<DISK_PAGE_BITS>(offset);
        rd_offset.spare = 0;
        buffer_off = uint16_t(offset.offset - rd_offset.offset);
        // spare bits are number of pages needed to load node
        auto const num_pages_to_load_node = offset.spare;
        MONAD_DEBUG_ASSERT(
            num_pages_to_load_node <=
            round_up_align<DISK_PAGE_BITS>(Node::max_disk_size));
        bytes_to_read =
            static_cast<unsigned>(num_pages_to_load_node << DISK_PAGE_BITS);
        MONAD_ASSERT(bytes_to_read <= AsyncIO::READ_BUFFER_SIZE);
    }

    void set_value(
        erased_connected_operation *,
        read_single_buffer_sender::result_type buffer_)
    {
        MONAD_ASSERT(buffer_);
        auto &buffer = buffer_.assume_value().get();
        // load node from read buffer
        auto *parent = tnode->parent;
        MONAD_DEBUG_ASSERT(parent);
        auto &entry = parent->children[tnode->child_index()];
        MONAD_DEBUG_ASSERT(entry.branch < 16);
        auto &child = tnode->children[bitmask_index(
            tnode->orig_mask,
            static_cast<unsigned>(std::countr_zero(tnode->mask)))];
        child.ptr = deserialize_node_from_buffer(
                        (unsigned char *)buffer.data() + buffer_off,
                        buffer.size() - buffer_off)
                        .release();
        buffer.reset();
        auto const path_size = tnode->path.nibble_size();
        create_node_compute_data_possibly_async(
            *aux, *sm, *parent, entry, tnode_unique_ptr{tnode}, false);
        sm->up(path_size + !parent->is_sentinel());
        upward_update(*aux, *sm, parent);
    }
};

static_assert(sizeof(read_single_child_receiver) == 48);
static_assert(alignof(read_single_child_receiver) == 8);

struct compaction_receiver
{
    UpdateAux *aux;
    chunk_offset_t rd_offset;
    chunk_offset_t orig_offset;
    CompactTNode *tnode;
    uint8_t index;
    unsigned bytes_to_read;
    uint16_t buffer_off;
    std::unique_ptr<StateMachine> sm;

    compaction_receiver(
        UpdateAux *aux_, std::unique_ptr<StateMachine> sm_,
        CompactTNode *tnode_, unsigned const index_,
        chunk_offset_t const offset)
        : aux(aux_)
        , rd_offset({0, 0})
        , orig_offset(offset)
        , tnode(tnode_)
        , index(static_cast<uint8_t>(index_))
        , sm(std::move(sm_))
    {
        MONAD_ASSERT(tnode);
        MONAD_ASSERT(tnode->npending > 0 && tnode->npending <= 16);
        rd_offset =
            round_down_align<DISK_PAGE_BITS>(aux->virtual_to_physical(offset));
        auto const num_pages_to_load_node = rd_offset.spare;
        buffer_off = uint16_t(offset.offset - rd_offset.offset);
        MONAD_DEBUG_ASSERT(
            num_pages_to_load_node <=
            round_up_align<DISK_PAGE_BITS>(Node::max_disk_size));
        bytes_to_read =
            static_cast<unsigned>(num_pages_to_load_node << DISK_PAGE_BITS);
        rd_offset.spare = 0;

        aux_->collect_compaction_read_stats(offset, bytes_to_read);
    }

    void set_value(
        erased_connected_operation *,
        monad::async::read_single_buffer_sender::result_type buffer_)
    {
        MONAD_ASSERT(buffer_);
        auto &buffer = buffer_.assume_value().get();
        auto *node = deserialize_node_from_buffer(
                         (unsigned char *)buffer.data() + buffer_off,
                         buffer.size() - buffer_off)
                         .release();
        buffer.reset();
        compact_(*aux, *sm, tnode, index, node, false, orig_offset);
        // now if tnode has everything it needs
        while (!tnode->npending) {
            if (tnode->type == tnode_type::update) {
                upward_update(*aux, *sm, (UpwardTreeNode *)tnode);
                return;
            }
            auto *parent = tnode->parent;
            MONAD_ASSERT(parent);
            // tnode is freed after this call
            try_fillin_parent_with_rewritten_node(
                *aux, CompactTNode::unique_ptr_type{tnode});
            // upward by one level
            tnode = parent;
        }
    }
};

template <receiver Receiver>
void async_read(UpdateAux &aux, Receiver &&receiver)
{
    read_update_sender sender(receiver);
    auto iostate =
        aux.io->make_connected(std::move(sender), std::move(receiver));
    iostate->initiate();
    // TEMPORARY UNTIL ALL THIS GETS BROKEN OUT: Release
    // management until i/o completes
    iostate.release();
}

/////////////////////////////////////////////////////
// Create Node
/////////////////////////////////////////////////////
Node *create_node_from_children_if_any(
    UpdateAux &aux, StateMachine &sm, uint16_t const orig_mask,
    uint16_t const mask, std::span<ChildData> children, NibblesView const path,
    std::optional<byte_string_view> const leaf_data = std::nullopt)
{
    aux.collect_number_nodes_created_stats();
    // handle non child and single child cases
    auto const number_of_children = static_cast<unsigned>(std::popcount(mask));
    if (number_of_children == 0) {
        return leaf_data.has_value()
                   ? make_node(0, {}, path, leaf_data.value(), {}).release()
                   : nullptr;
    }
    else if (number_of_children == 1 && !leaf_data.has_value()) {
        auto const j = bitmask_index(
            orig_mask, static_cast<unsigned>(std::countr_zero(mask)));
        MONAD_DEBUG_ASSERT(children[j].ptr);
        auto const node = Node::UniquePtr{children[j].ptr};
        /* Note: there's a potential superfluous extension hash recomputation
        when node coaleases upon erases, because we compute node hash when path
        is not yet the final form. There's not yet a good way to avoid this
        unless we delay all the compute() after all child branches finish
        creating nodes and return in the recursion */
        return make_node(
                   *node,
                   concat(path, children[j].branch, node->path_nibble_view()),
                   node->has_value() ? std::make_optional(node->value())
                                     : std::nullopt)
            .release();
    }
    MONAD_DEBUG_ASSERT(
        number_of_children > 1 ||
        (number_of_children == 1 && leaf_data.has_value()));
    // write children to disk, free any if exceeds the cache level limit
    if (aux.is_on_disk()) {
        for (auto &child : children) {
            if (child.is_valid() && child.offset == INVALID_OFFSET) {
                // write updated node or node to be compacted to disk
                // won't duplicate write of unchanged old child
                MONAD_DEBUG_ASSERT(child.branch < 16);
                MONAD_DEBUG_ASSERT(child.ptr);
                child.offset =
                    async_write_node_set_spare(aux, *child.ptr, true);
                std::tie(child.min_offset_fast, child.min_offset_slow) =
                    calc_min_offsets(*child.ptr, child.offset);
                if (sm.compact()) {
                    MONAD_DEBUG_ASSERT(
                        child.min_offset_fast >= aux.compact_offset_fast);
                    MONAD_DEBUG_ASSERT(
                        child.min_offset_slow >= aux.compact_offset_slow);
                }
            }
            // apply cache based on state machine state, always cache node that
            // is a single child
            if (child.ptr && number_of_children > 1 && !child.cache_node) {
                {
                    Node::UniquePtr const _{child.ptr};
                }
                child.ptr = nullptr;
            }
        }
    }
    return create_node(sm.get_compute(), mask, children, path, leaf_data);
}

void create_node_compute_data_possibly_async(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    tnode_unique_ptr tnode, bool const might_on_disk)
{
    if (might_on_disk && tnode->number_of_children() == 1) {
        auto &child = tnode->children[bitmask_index(
            tnode->orig_mask,
            static_cast<unsigned>(std::countr_zero(tnode->mask)))];
        if (!child.ptr) {
            MONAD_DEBUG_ASSERT(aux.is_on_disk());
            read_single_child_receiver receiver(
                &aux, sm.clone(), tnode.release(), child);
            async_read(aux, std::move(receiver));
            MONAD_DEBUG_ASSERT(parent.npending);
            return;
        }
    }
    Node *node = create_node_from_children_if_any(
        aux,
        sm,
        tnode->orig_mask,
        tnode->mask,
        tnode->children,
        tnode->path,
        tnode->opt_leaf_data);
    MONAD_DEBUG_ASSERT(entry.branch < 16);
    if (node) {
        entry.finalize(*node, sm.get_compute(), sm.cache());
    }
    else {
        parent.mask &=
            static_cast<uint16_t>(~(static_cast<uint16_t>(1u << entry.branch)));
        entry.erase();
    }
    --parent.npending;
}

void update_value_and_subtrie_(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    Node::UniquePtr old, NibblesView const path, Update &update)
{
    if (update.is_deletion()) {
        parent.mask &= static_cast<uint16_t>(~(1u << entry.branch));
        entry.erase();
        MONAD_DEBUG_ASSERT(entry.ptr == nullptr);
        --parent.npending;
        return;
    }
    // No need to check next is empty or not, following branches will handle it
    Requests requests;
    requests.split_into_sublists(std::move(update.next), 0);
    MONAD_ASSERT(requests.opt_leaf == std::nullopt);
    if (update.incarnation) {
        // handles empty requests sublist too
        create_new_trie_from_requests_(
            aux, sm, entry, requests, path, 0, update.value);
        --parent.npending;
    }
    else {
        auto const opt_leaf =
            update.value.has_value() ? update.value : old->opt_value();
        // TODO will check if children need compaction
        dispatch_updates_impl_(
            aux,
            sm,
            parent,
            entry,
            std::move(old),
            requests,
            0,
            path,
            opt_leaf);
    }
    return;
}

/////////////////////////////////////////////////////
// Create a new trie from a list of updates, no incarnation
/////////////////////////////////////////////////////
void create_new_trie_(
    UpdateAux &aux, StateMachine &sm, ChildData &entry, UpdateList &&updates,
    unsigned prefix_index)
{
    if (updates.empty()) {
        return;
    }
    if (updates.size() == 1) {
        Update &update = updates.front();
        MONAD_DEBUG_ASSERT(update.value.has_value());
        auto const path = update.key.substr(prefix_index);
        for (auto i = 0u; i < path.nibble_size(); ++i) {
            sm.down(path.get(i));
        }
        if (!update.next.empty()) { // nested updates
            MONAD_DEBUG_ASSERT(update.value.has_value());
            Requests requests;
            requests.split_into_sublists(std::move(update.next), 0);
            create_new_trie_from_requests_(
                aux, sm, entry, requests, path, 0, update.value);
        }
        else {
            aux.collect_number_nodes_created_stats();
            entry.finalize(
                *make_node(0, {}, path, update.value.value(), {}).release(),
                sm.get_compute(),
                sm.cache());
        }

        if (path.nibble_size()) {
            sm.up(path.nibble_size());
        }
        return;
    }
    Requests requests;
    auto const prefix_index_start = prefix_index;
    while (requests.split_into_sublists(
               std::move(updates), prefix_index) == // NOLINT
               1 &&
           !requests.opt_leaf) {
        sm.down(requests.get_first_branch());
        updates = std::move(requests).first_and_only_list();
        ++prefix_index;
    }
    create_new_trie_from_requests_(
        aux,
        sm,
        entry,
        requests,
        requests.get_first_path().substr(
            prefix_index_start, prefix_index - prefix_index_start),
        prefix_index,
        requests.opt_leaf.and_then(&Update::value));
    if (prefix_index_start != prefix_index) {
        sm.up(prefix_index - prefix_index_start);
    }
}

void create_new_trie_from_requests_(
    UpdateAux &aux, StateMachine &sm, ChildData &entry, Requests &requests,
    NibblesView const path, unsigned const prefix_index,
    std::optional<byte_string_view> const opt_leaf_data)
{
    auto const number_of_children =
        static_cast<unsigned>(std::popcount(requests.mask));
    uint16_t const mask = requests.mask;
    allocators::owning_span<ChildData> const children(number_of_children);
    for (unsigned i = 0, j = 0, bit = 1; j < number_of_children;
         ++i, bit <<= 1) {
        if (bit & requests.mask) {
            children[j] = ChildData{.branch = static_cast<uint8_t>(i)};
            sm.down(children[j].branch);
            create_new_trie_(
                aux, sm, children[j], std::move(requests)[i], prefix_index + 1);
            sm.up(1);
            ++j;
        }
    }
    // can have empty children
    auto *node = create_node_from_children_if_any(
        aux, sm, mask, mask, children, path, opt_leaf_data);
    MONAD_ASSERT(node);
    entry.finalize(*node, sm.get_compute(), sm.cache());
}

/////////////////////////////////////////////////////
// Update existing subtrie
/////////////////////////////////////////////////////

void upsert_(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    Node::UniquePtr old, chunk_offset_t const old_offset, UpdateList &&updates,
    unsigned prefix_index, unsigned old_prefix_index)
{
    if (!old) {
        update_receiver receiver(
            &aux,
            sm.clone(),
            entry,
            old_offset,
            std::move(updates),
            &parent,
            prefix_index);
        async_read(aux, std::move(receiver));
        return;
    }
    if (old_prefix_index == INVALID_PATH_INDEX) {
        old_prefix_index = old->path_start_nibble();
        MONAD_DEBUG_ASSERT(old_prefix_index != INVALID_PATH_INDEX);
    }
    unsigned const old_prefix_index_start = old_prefix_index;
    auto const prefix_index_start = prefix_index;
    Requests requests;
    while (true) {
        NibblesView path{
            old_prefix_index_start, old_prefix_index, old->path_data()};
        auto const update_size = updates.size();
        if (update_size == 1 &&
            prefix_index == updates.front().key.nibble_size()) {
            auto &update = updates.front();
            MONAD_ASSERT(old->path_nibble_index_end == old_prefix_index);
            MONAD_ASSERT(old->has_value());
            update_value_and_subtrie_(
                aux, sm, parent, entry, std::move(old), path, update);
            break;
        }
        unsigned const number_of_sublists = requests.split_into_sublists(
            std::move(updates), prefix_index); // NOLINT
        if (!update_size) { // no updates at all
            for (unsigned n = 0;
                 n < old->path_nibble_index_end - old_prefix_index;
                 ++n) { // continue going down to the end of old path
                sm.down(old->path_nibble_view().get(n));
            }
            MONAD_ASSERT(number_of_sublists == 0);
            MONAD_ASSERT(requests.opt_leaf == std::nullopt);
            prefix_index += old->path_nibble_index_end - old_prefix_index;
            old_prefix_index = old->path_nibble_index_end;
            path = NibblesView{
                old_prefix_index_start, old_prefix_index, old->path_data()};
            // will just call dispatch and break;
        }
        if (old_prefix_index == old->path_nibble_index_end) {
            dispatch_updates_flat_list_(
                aux,
                sm,
                parent,
                entry,
                std::move(old),
                requests,
                path,
                prefix_index);
            break;
        }
        if (auto old_nibble = get_nibble(old->path_data(), old_prefix_index);
            number_of_sublists == 1 &&
            requests.get_first_branch() == old_nibble) {
            MONAD_DEBUG_ASSERT(requests.opt_leaf == std::nullopt);
            updates = std::move(requests)[old_nibble];
            sm.down(old_nibble);
            ++prefix_index;
            ++old_prefix_index;
            continue;
        }
        // meet a mismatch or split, not till the end of old path
        mismatch_handler_(
            aux,
            sm,
            parent,
            entry,
            std::move(old),
            requests,
            path,
            old_prefix_index,
            prefix_index);
        break;
    }
    if (prefix_index_start != prefix_index) {
        sm.up(prefix_index - prefix_index_start);
    }
}

void fillin_entry(
    UpdateAux &aux, StateMachine &sm, tnode_unique_ptr tnode,
    UpwardTreeNode &parent, ChildData &entry)
{
    if (tnode->npending) {
        tnode.release();
    }
    else {
        create_node_compute_data_possibly_async(
            aux, sm, parent, entry, std::move(tnode));
    }
}

/* dispatch updates at the end of old node's path. old node may have leaf data,
 * and there might be update to the leaf value. */
void dispatch_updates_impl_(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    Node::UniquePtr old_ptr, Requests &requests, unsigned const prefix_index,
    NibblesView const path, std::optional<byte_string_view> const opt_leaf_data)
{
    Node *old = old_ptr.get();
    uint16_t const orig_mask = old->mask | requests.mask;
    auto const number_of_children =
        static_cast<unsigned>(std::popcount(orig_mask));
    auto tnode = make_tnode(
        orig_mask,
        prefix_index,
        &parent,
        entry.branch,
        path,
        opt_leaf_data,
        opt_leaf_data.has_value() ? std::move(old_ptr) : Node::UniquePtr{});
    MONAD_DEBUG_ASSERT(tnode->children.size() == number_of_children);
    auto &children = tnode->children;

    for (unsigned i = 0, j = 0, bit = 1; j < number_of_children;
         ++i, bit <<= 1) {
        MONAD_DEBUG_ASSERT(i <= std::numeric_limits<uint8_t>::max());
        if (bit & requests.mask) {
            children[j] = ChildData{.branch = static_cast<uint8_t>(i)};
            sm.down(children[j].branch);
            if (bit & old->mask) {
                upsert_(
                    aux,
                    sm,
                    *tnode,
                    children[j],
                    old->next_ptr(old->to_child_index(i)),
                    old->fnext(old->to_child_index(i)),
                    std::move(requests)[i],
                    prefix_index + 1,
                    INVALID_PATH_INDEX);
                sm.up(1);
            }
            else {
                create_new_trie_(
                    aux,
                    sm,
                    children[j],
                    std::move(requests)[i],
                    prefix_index + 1);
                --tnode->npending;
                sm.up(1);
            }
            ++j;
        }
        else if (bit & old->mask) {
            auto &child = children[j];
            child.copy_old_child(old, i);
            auto const old_index = old->to_child_index(i);
            auto const orig_child_offset = old->fnext(old_index);
            if (aux.is_on_disk() && sm.compact() &&
                (old->min_offset_fast(old_index) < aux.compact_offset_fast ||
                 old->min_offset_slow(old_index) < aux.compact_offset_slow)) {
                aux.collect_compacted_nodes_stats(
                    old->min_offset_fast(old_index),
                    old->min_offset_slow(old_index));
                child.offset = INVALID_OFFSET; // to be rewritten
                compact_(
                    aux,
                    sm,
                    reinterpret_cast<CompactTNode *>(tnode.get()),
                    j,
                    child.ptr,
                    true,
                    orig_child_offset);
            }
            else {
                --tnode->npending;
            }
            ++j;
        }
    }
    fillin_entry(aux, sm, std::move(tnode), parent, entry);
}

void dispatch_updates_flat_list_(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    Node::UniquePtr old, Requests &requests, NibblesView const path,
    unsigned prefix_index)
{
    auto &opt_leaf = requests.opt_leaf;
    auto opt_leaf_data = old->opt_value();
    if (opt_leaf.has_value()) {
        MONAD_ASSERT(opt_leaf->next.empty());
        if (opt_leaf.value().incarnation) {
            // incarnation means there are new children keys longer than
            // curr update's key
            MONAD_DEBUG_ASSERT(!opt_leaf.value().is_deletion());
            create_new_trie_from_requests_(
                aux,
                sm,
                entry,
                requests,
                path,
                prefix_index,
                opt_leaf.value().value);
            --parent.npending;
            return;
        }
        else if (opt_leaf.value().is_deletion()) {
            parent.mask &= static_cast<uint16_t>(~(1u << entry.branch));
            entry.erase();
            --parent.npending;
            return;
        }
        if (opt_leaf.value().value.has_value()) {
            opt_leaf_data = opt_leaf.value().value;
        }
    }
    dispatch_updates_impl_(
        aux,
        sm,
        parent,
        entry,
        std::move(old),
        requests,
        prefix_index,
        path,
        opt_leaf_data);
}

// Split `old` at old_prefix_index, `updates` are already splitted at
// prefix_index to `requests`, which can have 1 or more sublists.
void mismatch_handler_(
    UpdateAux &aux, StateMachine &sm, UpwardTreeNode &parent, ChildData &entry,
    Node::UniquePtr old_ptr, Requests &requests, NibblesView const path,
    unsigned const old_prefix_index, unsigned const prefix_index)
{
    Node &old = *old_ptr;
    MONAD_DEBUG_ASSERT(old.has_path());
    // Note: no leaf can be created at an existing non-leaf node
    MONAD_DEBUG_ASSERT(!requests.opt_leaf.has_value());
    unsigned char const old_nibble =
        get_nibble(old.path_data(), old_prefix_index);
    uint16_t const orig_mask =
        static_cast<uint16_t>(1u << old_nibble | requests.mask);
    auto tnode =
        make_tnode(orig_mask, prefix_index, &parent, entry.branch, path);
    auto const number_of_children =
        static_cast<unsigned>(std::popcount(orig_mask));
    MONAD_DEBUG_ASSERT(
        tnode->children.size() == number_of_children && number_of_children > 0);
    auto &children = tnode->children;

    for (unsigned i = 0, j = 0, bit = 1; j < number_of_children;
         ++i, bit <<= 1) {
        if (bit & requests.mask) {
            children[j] = ChildData{.branch = static_cast<uint8_t>(i)};
            sm.down(children[j].branch);
            if (i == old_nibble) {
                upsert_(
                    aux,
                    sm,
                    *tnode,
                    children[j],
                    std::move(old_ptr),
                    INVALID_OFFSET,
                    std::move(requests)[i],
                    prefix_index + 1,
                    old_prefix_index + 1);
            }
            else {
                create_new_trie_(
                    aux,
                    sm,
                    children[j],
                    std::move(requests)[i],
                    prefix_index + 1);
                --tnode->npending;
            }
            ++j;
            sm.up(1);
        }
        else if (i == old_nibble) {
            sm.down(old_nibble);
            // nexts[j] is a path-shortened old node, trim prefix
            NibblesView const path_suffix{
                old_prefix_index + 1,
                old.path_nibble_index_end,
                old.path_data()};
            for (auto i = 0u; i < path_suffix.nibble_size(); ++i) {
                sm.down(path_suffix.get(i));
            }
            auto &child = children[j];
            child = ChildData{.branch = static_cast<uint8_t>(i)};
            child.finalize(
                *make_node(old, path_suffix, old.opt_value()).release(),
                sm.get_compute(),
                sm.cache());
            sm.up(path_suffix.nibble_size() + 1);
            if (auto const [min_offset_fast, min_offset_slow] =
                    calc_min_offsets(*child.ptr);
                aux.is_on_disk() && sm.compact() &&
                (min_offset_fast < aux.compact_offset_fast ||
                 min_offset_slow < aux.compact_offset_slow)) {
                aux.collect_compacted_nodes_stats(
                    min_offset_fast, min_offset_slow);
                compact_(
                    aux, sm, (CompactTNode *)tnode.get(), j, child.ptr, true);
            }
            else {
                --tnode->npending;
            }
            ++j;
        }
    }
    fillin_entry(aux, sm, std::move(tnode), parent, entry);
}

void compact_(
    UpdateAux &aux, StateMachine &sm, CompactTNode *const parent,
    unsigned const index, Node *const node, bool const cached,
    chunk_offset_t const node_offset)
{
    if (!node) {
        compaction_receiver receiver(
            &aux, sm.clone(), parent, index, node_offset);
        async_read(aux, std::move(receiver));
        return;
    }
    bool const rewrite_to_fast = /* INVALID_OFFSET also falls here */
        (node_offset.get_highest_bit() /*in fast list*/ &&
         detail::compact_chunk_offset_t{node_offset} >=
             aux.compact_offset_fast);
    auto tnode =
        CompactTNode::make(parent, index, node, rewrite_to_fast, cached);

    aux.collect_compacted_nodes_from_to_stats(node_offset, rewrite_to_fast);

    for (unsigned j = 0; j < node->number_of_children(); ++j) {
        if (node->min_offset_fast(j) < aux.compact_offset_fast ||
            node->min_offset_slow(j) < aux.compact_offset_slow) {
            aux.collect_compacted_nodes_stats(
                node->min_offset_fast(j), node->min_offset_slow(j));
            compact_(
                aux, sm, tnode.get(), j, node->next(j), true, node->fnext(j));
        }
        else {
            --tnode->npending;
        }
    }
    // Compaction below `node` is completed, rewrite `node` to disk and put
    // offset and min_offset somewhere in parent depends on its type
    try_fillin_parent_with_rewritten_node(aux, std::move(tnode));
}

void try_fillin_parent_with_rewritten_node(
    UpdateAux &aux, CompactTNode::unique_ptr_type tnode)
{
    if (tnode->npending) { // there are unfinished async below node
        tnode.release();
        return;
    }
    auto const new_offset =
        async_write_node_set_spare(aux, *tnode->node, tnode->rewrite_to_fast);
    auto const [min_offset_fast, min_offset_slow] =
        calc_min_offsets(*tnode->node, new_offset);
    MONAD_DEBUG_ASSERT(min_offset_fast >= aux.compact_offset_fast);
    MONAD_DEBUG_ASSERT(min_offset_slow >= aux.compact_offset_slow);
    auto *parent = tnode->parent;
    auto const index = tnode->index;
    if (parent->type == tnode_type::copy) {
        parent->node->set_fnext(index, new_offset);
        parent->node->set_min_offset_fast(index, min_offset_fast);
        parent->node->set_min_offset_slow(index, min_offset_slow);
        if (tnode->cached) { // debug
            MONAD_DEBUG_ASSERT(parent->node->next(index) == tnode->node);
        }
    }
    else { // parent tnode is an update tnode
        auto *p = reinterpret_cast<UpwardTreeNode *>(parent);
        p->children[index].offset = new_offset;
        p->children[index].min_offset_fast = min_offset_fast;
        p->children[index].min_offset_slow = min_offset_slow;
        if (tnode->cached) { // debug
            MONAD_DEBUG_ASSERT(p->children[index].ptr == tnode->node);
        }
    }
    --parent->npending;
}

/////////////////////////////////////////////////////
// Async write
/////////////////////////////////////////////////////
node_writer_unique_ptr_type replace_node_writer(
    UpdateAux &aux, node_writer_unique_ptr_type &node_writer,
    size_t bytes_yet_to_be_appended_to_existing,
    size_t bytes_to_write_to_new_writer)
{
    // Can't use add_to_offset(), because it asserts if we go past the
    // capacity
    auto offset_of_next_block = node_writer->sender().offset();
    bool const in_fast_list =
        aux.db_metadata()->at(offset_of_next_block.id)->in_fast_list;
    file_offset_t offset = offset_of_next_block.offset;
    offset += node_writer->sender().written_buffer_bytes() +
              bytes_yet_to_be_appended_to_existing;
    offset_of_next_block.offset = offset & chunk_offset_t::max_offset;
    auto block_size = AsyncIO::WRITE_BUFFER_SIZE;
    auto const chunk_capacity = aux.io->chunk_capacity(offset_of_next_block.id);
    MONAD_ASSERT(offset <= chunk_capacity);
    if (offset == chunk_capacity ||
        offset + bytes_to_write_to_new_writer > chunk_capacity) {
        // If after the current write buffer we're hitting chunk capacity or the
        // remaining bytes in current chunk not enough for the second half of
        // node, we replace writer to the start of next chunk.
        auto const *ci_ = aux.db_metadata()->free_list_end();
        MONAD_ASSERT(ci_ != nullptr); // we are out of free blocks!
        auto idx = ci_->index(aux.db_metadata());
        aux.remove(idx);
        aux.append(
            in_fast_list ? UpdateAux::chunk_list::fast
                         : UpdateAux::chunk_list::slow,
            idx);
        offset_of_next_block.id = idx & 0xfffffU;
        offset_of_next_block.offset = 0;
    }
    else if (offset + block_size > chunk_capacity) {
        block_size = chunk_capacity - offset;
    }
    auto ret = aux.io->make_connected(
        write_single_buffer_sender{offset_of_next_block, block_size},
        write_operation_io_receiver{});
    return ret;
}

// return physical offset the node is written at
async_write_node_result async_write_node(
    UpdateAux &aux, node_writer_unique_ptr_type &node_writer, Node const &node)
{
    aux.io->poll_nonblocking_if_not_within_completions(1);
    auto *sender = &node_writer->sender();
    auto const size = node.disk_size;
    auto const remaining_bytes = sender->remaining_buffer_bytes();
    async_write_node_result ret{
        .offset_written_to = INVALID_OFFSET,
        .bytes_appended = size,
        .io_state = node_writer.get()};
    [[likely]] if (size <= remaining_bytes) {
        ret.offset_written_to =
            sender->offset().add_to_offset(sender->written_buffer_bytes());
        auto *where_to_serialize = sender->advance_buffer_append(size);
        MONAD_DEBUG_ASSERT(where_to_serialize != nullptr);
        serialize_node_to_buffer((unsigned char *)where_to_serialize, node);
    }
    else {
        // renew write sender
        auto new_node_writer = replace_node_writer(
            aux, node_writer, remaining_bytes, size - remaining_bytes);
        auto *new_sender = &new_node_writer->sender();
        auto *where_to_serialize = (unsigned char *)new_sender->buffer().data();
        MONAD_DEBUG_ASSERT(where_to_serialize != nullptr);
        serialize_node_to_buffer(where_to_serialize, node);
        // Corner case bug is avoided: when remaining_bytes = 0 and we reach the
        // end of chunk, which can happen inside else{} branch, if we use
        // add_to_offset(), it will exceed max_offset value and trigger the
        // assertion.
        if (new_sender->offset().id == sender->offset().id) {
            // In this branch, current node_writer won't be writing to the end
            // of chunk.
            ret.offset_written_to =
                sender->offset().add_to_offset(sender->written_buffer_bytes());
            // Move the front of new_sender into the tail of sender as they
            // share the same chunk
            auto *where_to_serialize2 =
                sender->advance_buffer_append(remaining_bytes);
            MONAD_DEBUG_ASSERT(where_to_serialize2 != nullptr);
            memcpy(where_to_serialize2, where_to_serialize, remaining_bytes);
            memmove(
                where_to_serialize,
                where_to_serialize + remaining_bytes,
                size - remaining_bytes);
            MONAD_ASSERT(
                new_sender->advance_buffer_append(size - remaining_bytes) !=
                nullptr);
        }
        else {
            // Don't split nodes across storage chunks, this simplifies reads
            // greatly
            MONAD_ASSERT(new_sender->written_buffer_bytes() == 0);
            MONAD_ASSERT(size <= new_sender->remaining_buffer_bytes());
            ret.offset_written_to = new_sender->offset();
            ret.io_state = new_node_writer.get();
            MONAD_ASSERT(new_sender->advance_buffer_append(size) != nullptr);
            // Pad buffer about to get initiated so it's O_DIRECT i/o aligned
            auto *tozero = sender->advance_buffer_append(remaining_bytes);
            MONAD_DEBUG_ASSERT(tozero != nullptr);
            memset(tozero, 0, remaining_bytes);
        }
        auto to_initiate = std::move(node_writer);
        node_writer = std::move(new_node_writer);
        to_initiate->initiate();
        // shall be recycled by the i/o receiver
        to_initiate.release();
    }
    return ret;
}

// Return node's virtual offset the node is written at
// hide the physical offset detail from triedb
chunk_offset_t
async_write_node_set_spare(UpdateAux &aux, Node &node, bool write_to_fast)
{
    if (aux.alternate_slow_fast_writer) {
        // alternate between slow and fast writer
        write_to_fast &= aux.can_write_to_fast;
        aux.can_write_to_fast = !aux.can_write_to_fast;
    }
    auto off = async_write_node(
                   aux,
                   write_to_fast ? aux.node_writer_fast : aux.node_writer_slow,
                   node)
                   .offset_written_to;
    auto const pages = num_pages(off.offset, node.get_disk_size());
    MONAD_DEBUG_ASSERT(pages <= std::numeric_limits<uint16_t>::max());
    off.spare = static_cast<uint16_t>(pages);
    return aux.physical_to_virtual(off);
}

// return virtual offset of root
chunk_offset_t write_new_root_node(UpdateAux &aux, Node &root)
{
    auto const virtual_offset_written_to =
        async_write_node_set_spare(aux, root, true);
    // Round up with all bits zero
    auto replace = [&](node_writer_unique_ptr_type &node_writer) {
        auto *sender = &node_writer->sender();
        auto written = sender->written_buffer_bytes();
        auto paddedup = round_up_align<DISK_PAGE_BITS>(written);
        auto const tozerobytes = paddedup - written;
        auto *tozero = sender->advance_buffer_append(tozerobytes);
        MONAD_DEBUG_ASSERT(tozero != nullptr);
        memset(tozero, 0, tozerobytes);
        // replace fast node writer
        auto new_node_writer = replace_node_writer(aux, node_writer);
        auto to_initiate = std::move(node_writer);
        node_writer = std::move(new_node_writer);
        to_initiate->initiate();
        // shall be recycled by the i/o receiver
        to_initiate.release();
    };
    replace(aux.node_writer_fast);
    if (aux.node_writer_slow->sender().written_buffer_bytes()) {
        // replace slow node writer
        replace(aux.node_writer_slow);
    }
    // flush async write root and slow writer
    aux.io->flush();
    // write new root offset and slow ring's latest offset to the front of disk
    aux.advance_offsets_to(
        aux.virtual_to_physical(virtual_offset_written_to),
        aux.node_writer_fast->sender().offset(),
        aux.node_writer_slow->sender().offset());
    return virtual_offset_written_to;
}

MONAD_MPT_NAMESPACE_END
